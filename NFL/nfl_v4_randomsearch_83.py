# -*- coding: utf-8 -*-
"""nfl-v4-RandomSearch-83.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1H3mu4eT48iNPyzAimkbMZWsbMMDuaCi1
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# カテゴリ変数を数値化に変換するエンコーダ
from sklearn.preprocessing import LabelEncoder
# ランダムフォレストによる分類器
from sklearn.ensemble import RandomForestClassifier
# 層化K分割交差検証を行うクラス
from sklearn.model_selection import StratifiedKFold, RandomizedSearchCV
# ROC　AUC　スコアを計算する評価指標
from sklearn.metrics import roc_auc_score
from scipy.stats import randint, uniform

PATH = '/content/drive/My Drive/GCI/NFL/'

train = pd.read_csv(PATH + 'train.csv')
test = pd.read_csv(PATH + 'test.csv')

# print('Train', train.shape)
# print('Test', test.shape)

train.head()
# train.info()
# train.isnull().sum()
# test.isnull().sum()

drafted_counts = train['Drafted'].value_counts()

plt.figure(figsize=(8,6))
plt.bar(drafted_counts.index.astype(str), drafted_counts.values)
plt.title('Drafted Distribution')
plt.xlabel('Drafted', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

drafted_percentage = train['Drafted'].value_counts(normalize=True) * 100
# drafted_percentage
print(f'Percentage of 0: {drafted_percentage.get(0, 0):.2f}%')
print(f'Percentage of 1: {drafted_percentage.get(1, 0):.2f}%')

# 数値列だけを取り出す（ID,Drafted列は除く）
numeric_cols = train.select_dtypes(include=['number']).columns
numeric_cols = numeric_cols.drop(['Id','Drafted'])
# numeric_cols

# プロット
num_cols = len(numeric_cols)
cols = 3
rows = (num_cols + cols - 1) // cols

plt.figure(figsize=(5 * cols, 4 * rows))

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(rows, cols, i)
    plt.hist(train[col].dropna(), bins=30, edgecolor='black')
    plt.title(f'Histgram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# 数値列だけを取り出す（Id,Drafted列を除く）
numeric_cols = train.select_dtypes(include=['number']).drop(['Id','Drafted'], axis=1)

# 相関行列を計算
corr_matrix = numeric_cols.corr()
# corr_matrix

# ヒートマップをプロット
plt.figure(figsize=(12, 10))
sns.heatmap(
    corr_matrix,
    annot=True,
    cmap='coolwarm',
    vmin=-1, vmax=1,
    square=True,
    linewidths=0.5
)
plt.title('Correlation Heatmap')
plt.show()

# 「Sprint_40yd」列と「Broad_Jump」列の相関係数は-0.82で、強い負の相関関係が見られました
# 箱ひげ図を描画
plt.figure(figsize=(10, 6))
sns.boxplot(x=train['Sprint_40yd'])
plt.title('Boxplot of Sprint_40yd', fontsize=16)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()
# 「なぜ負の相関があるのか？」という疑問を解決する

# カテゴリデータを抽出
categorical_cols = train.select_dtypes(include=['object', 'category']).columns

# 各列の水準数を獲得
levels_count = {col: train[col].nunique() for col in categorical_cols}

# for col, count in levels_count.items():
#   print(f'{col}: {count} levels')

# Schoolは水準数が236とかなり多く、可視化すると潰れて見づらいのでここでは可視化しません。

# カテゴリデータを抽出
categorical_cols = train.select_dtypes(include=['object', 'category']).columns
categorical_cols = categorical_cols.drop('School')

# グラフ描画準備
num_cols = len(categorical_cols)
rows = 1
cols = num_cols

fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5))

if cols == 1:
  axes = [axes]
else:
  axes = axes.flatten()

# 各カテゴリ変数でカウントプロット
for i, col in enumerate(categorical_cols):
  sns.countplot(x=col, data=train, order=train[col].value_counts().index, ax=axes[i])
  axes[i].set_title(f'Count Plot of {col}', fontsize=14)
  axes[i].set_xlabel(col, fontsize=12)
  axes[i].set_ylabel('Count', fontsize=12)
  axes[i].tick_params(axis='x', rotation=45)
  axes[i].grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# カテゴリ変数（object型またはcategory型）を抽出し、school列を除外
categorical_cols = train.select_dtypes(include=['object', 'category']).columns
categorical_cols = categorical_cols.drop('School')

# グラフ描画準備
num_cols = len(categorical_cols)
rows = 1
cols = num_cols

fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5))

if cols == 1:
  axes = [axes]
else:
  axes = axes.flatten()

# 各カテゴリ変数ごとにDrafted列の平均を棒グラフ描画
for i, col in enumerate(categorical_cols):
  mean_values = train.groupby(col)['Drafted'].mean().sort_values(ascending=False)
  # print(mean_values.index,'---', mean_values.values)
  sns.barplot(x=mean_values.index, y=mean_values.values, ax=axes[i])
  axes[i].set_title(f'Average Drafted by {col}', fontsize=14)
  axes[i].set_xlabel(col, fontsize=12)
  axes[i].set_ylabel('Avarage Drafted', fontsize=12)
  axes[i].set_ylim(0, 1)
  axes[i].tick_params(axis='x', rotation=45)
  axes[i].grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

train.head()

PATH = '/content/drive/My Drive/GCI/NFL/'

train = pd.read_csv(PATH + 'train.csv')
test = pd.read_csv(PATH + 'test.csv')

# 前処理　preprocessing
# 欠損補完、エンコーディングを行います
# 注意！ここでは「school」を削除しましたがDraftedに関わりかも
# IdとDrafted関係がないと予想される、削除する

# School frequency
school_freq = train["School"].value_counts(normalize=True)
train["school_freq"] = train["School"].map(school_freq)
test["school_freq"]  = test["School"].map(school_freq).fillna(0)

# Check if 'Id' and 'School' columns exist before dropping
columns_to_drop = ['Id', 'Year', 'School']
train = train.drop(columns=[col for col in columns_to_drop if col in train.columns])
test = test.drop(columns=[col for col in columns_to_drop if col in test.columns])

cols_to_fill = ['Age', 'Sprint_40yd', 'Vertical_Jump', 'Bench_Press_Reps',
                'Broad_Jump', 'Agility_3cone', 'Shuttle']
# train の平均で train/test 両方を補完
for col in cols_to_fill:
    mean_value = train[col].mean()
    train[col] = train[col].fillna(mean_value)
    test[col] = test[col].fillna(mean_value)


# カテゴリデータをラベルエンコーディング
label_encoders = {}
# for c in ['Player_Type', 'Position_Type', 'Position']:
for c in ['Player_Type', 'Position_Type', 'Position']:
  label_encoders[c] = LabelEncoder()
  train[c] = label_encoders[c].fit_transform(train[c].astype(str))
  test[c] = label_encoders[c].transform(test[c].astype(str))

# BMI
train['BMI'] = round(train['Weight'] / train['Height'] ** 2, 2)
test['BMI'] = round(test['Weight'] / test['Height'] ** 2, 2)

#爆発力
# train['Explosiveness'] = round(train['Vertical_Jump'] + train['Broad_Jump'], 2)
# test['Explosiveness'] = round(test['Vertical_Jump'] + test['Broad_Jump'], 2)

#速度
train['Speed_Score'] = round((train['Weight'] * 200) / (train['Sprint_40yd'] ** 4), 2)
test['Speed_Score'] = round((test['Weight'] * 200) / (test['Sprint_40yd'] ** 4), 2)

train.head()

# 特徴量と目的変数に分ける
X = train.drop(columns=['Drafted'])
y = train['Drafted']

# ハイパーパラメータの分布を定義（RandomizedSearchCV用）
# param_distributions = {
#     'n_estimators': randint(50, 500),
#     'max_depth': [3, 5, 7, 10, 15, None],
#     'min_samples_split': randint(2, 20),
#     'min_samples_leaf': randint(1, 10),
#     'max_features': ['sqrt', 'log2', None],
#     'bootstrap': [True, False]
# }
# 0.8303

# さらに高速版（最重要パラメータのみ）
# param_distributions_ultra_fast = {
#     'n_estimators': [100, 200],             # 2つの選択肢
#     'max_depth': [5, 10, None],             # 3つの選択肢
#     'max_features': ['sqrt', None]          # 2つの選択肢
# }
# 0.8301

# 高速版：重要なパラメータのみに絞る
param_distributions_fast = {
    'n_estimators': [100, 150, 200],        # 3つの選択肢
    'max_depth': [5, 7, 10, None],          # 4つの選択肢
    'min_samples_split': [2, 5, 10],        # 3つの選択肢
    'max_features': ['sqrt', 'log2', None]   # 3つの選択肢
}

# CrossValidationの設定
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# RandomizedSearchCVでハイパーパラメータ最適化
print("ランダムサーチによるハイパーパラメータ最適化を実行中...")
rf = RandomForestClassifier(random_state=2025)

# RandomizedSearchCVの実行
random_search = RandomizedSearchCV(
    estimator=rf,
    param_distributions=param_distributions_fast,
    n_iter=12,  # 試行回数（多いほど良いパラメータが見つかる可能性が高い）
    cv=skf,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1,
    random_state=42
)

random_search.fit(X, y)

# 最適なパラメータを表示
print(f"最適なパラメータ: {random_search.best_params_}")
print(f"最適なCV AUC: {round(random_search.best_score_, 4)}")

# 最適化されたモデルでの評価
best_model = random_search.best_estimator_

# スコア・予測格納用
auc_scores = []
test_pred_proba_list = []

print("\n最適化されたモデルでの詳細な評価:")
# Stratified K-Fold による学習と評価
for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):
    print(f'Fold {fold + 1}')

    X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
    y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

    # 最適化されたモデルで学習
    best_model.fit(X_train, y_train)

     # バリデーション予測とスコア
    y_valid_pred_proba = best_model.predict_proba(X_valid)[:, 1]
    auc = roc_auc_score(y_valid, y_valid_pred_proba)
    auc_scores.append(auc)
    print(f'AUC: {round(auc, 4)}')

    # テストデータ予測を保存
    test_pred_proba = best_model.predict_proba(test)[:, 1]
    test_pred_proba_list.append(test_pred_proba)

# 平均AUCを表示
mean_auc = np.mean(auc_scores)
print(f'\n最終結果:')
print(f'Mean AUC: {round(mean_auc, 4)}')
print(f'AUC標準偏差: {round(np.std(auc_scores), 4)}')

# テスト予測の平均を計算
test_pred_proba_mean = np.mean(test_pred_proba_list, axis=0)

# 最適化されたパラメータの詳細を表示
print(f'\n使用されたパラメータ:')
for param, value in random_search.best_params_.items():
    print(f'  {param}: {value}')

# 上位5つのパラメータセットを表示
print(f'\n上位5つのパラメータセット:')
results = random_search.cv_results_
for i in range(min(5, len(results['mean_test_score']))):
    idx = np.argsort(results['mean_test_score'])[::-1][i]
    print(f'{i+1}. AUC: {round(results["mean_test_score"][idx], 4)}, '
          f'Params: {results["params"][idx]}')

# ファイル作成
test_pred_proba_mean
submission = pd.read_csv(PATH + 'sample_submission.csv')
submission['Drafted'] = test_pred_proba_mean
submission.to_csv(PATH + 'v4_RandomSearch.csv', index=False)