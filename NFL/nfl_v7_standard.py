# -*- coding: utf-8 -*-
"""nfl-v7-Claude.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kdLYW2mm-JVQm1j-8O4Euy6Q2lEWR2az
"""

# from google.colab import drive
# drive.mount('/content/drive')

import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import roc_auc_score
import xgboost as xgb
import lightgbm as lgb
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.preprocessing import StandardScaler, LabelEncoder

class NFLDraftEnsemble:
    """
    Advanced ensemble model for NFL Draft prediction
    """

    def __init__(self, random_state=2025):
        self.random_state = random_state
        self.models = {}
        self.meta_model = None
        self.scaler = StandardScaler()

    def create_base_models(self):
        """Create diverse base models for ensemble"""

        # XGBoost variants with different focuses
        self.models['XGB_Balanced'] = xgb.XGBClassifier(
            n_estimators=400,
            max_depth=6,
            learning_rate=0.08,
            subsample=0.85,
            colsample_bytree=0.85,
            reg_alpha=0.1,
            reg_lambda=1.2,
            random_state=self.random_state,
            eval_metric='auc'
        )

        self.models['XGB_Conservative'] = xgb.XGBClassifier(
            n_estimators=300,
            max_depth=4,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.3,
            reg_lambda=2.0,
            random_state=self.random_state,
            eval_metric='auc'
        )

        self.models['XGB_Aggressive'] = xgb.XGBClassifier(
            n_estimators=500,
            max_depth=8,
            learning_rate=0.12,
            subsample=0.9,
            colsample_bytree=0.9,
            reg_alpha=0.05,
            reg_lambda=0.8,
            random_state=self.random_state,
            eval_metric='auc'
        )

        # LightGBM variants
        self.models['LGB_Fast'] = lgb.LGBMClassifier(
            n_estimators=400,
            max_depth=7,
            learning_rate=0.1,
            subsample=0.85,
            colsample_bytree=0.85,
            reg_alpha=0.1,
            reg_lambda=0.1,
            num_leaves=80,
            random_state=self.random_state,
            verbose=-1
        )

        self.models['LGB_Deep'] = lgb.LGBMClassifier(
            n_estimators=600,
            max_depth=10,
            learning_rate=0.06,
            subsample=0.8,
            colsample_bytree=0.8,
            reg_alpha=0.2,
            reg_lambda=0.2,
            num_leaves=150,
            random_state=self.random_state,
            verbose=-1
        )

        # Extra Trees for diversity
        self.models['ExtraTrees'] = ExtraTreesClassifier(
            n_estimators=300,
            max_depth=12,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=self.random_state,
            n_jobs=-1
        )

    def train_stacking_ensemble(self, X, y, n_folds=5):
        """
        Train stacking ensemble with cross-validation
        """
        self.create_base_models()

        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=self.random_state)

        # Store out-of-fold predictions
        oof_predictions = np.zeros((len(X), len(self.models)))
        model_scores = {}

        print("Training base models...")

        for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):
            print(f"Fold {fold + 1}/{n_folds}")

            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]

            # Handle missing values
            X_train_filled = X_train.fillna(X_train.median())
            X_val_filled = X_val.fillna(X_train.median())

            for i, (model_name, model) in enumerate(self.models.items()):
                if fold == 0:
                    model_scores[model_name] = []

                # Train model
                model.fit(X_train_filled, y_train)

                # Predict validation set
                val_pred = model.predict_proba(X_val_filled)[:, 1]
                oof_predictions[val_idx, i] = val_pred

                # Calculate score
                score = roc_auc_score(y_val, val_pred)
                model_scores[model_name].append(score)

        # Print individual model performance
        print("\nBase model performance:")
        for model_name, scores in model_scores.items():
            mean_score = np.mean(scores)
            std_score = np.std(scores)
            print(f"{model_name:15}: {mean_score:.4f} (Â±{std_score:.4f})")

        # Train meta-model on out-of-fold predictions
        print("\nTraining meta-model...")

        # Scale features for logistic regression
        oof_scaled = self.scaler.fit_transform(oof_predictions)

        self.meta_model = LogisticRegression(
            C=1.0,
            random_state=self.random_state,
            max_iter=1000
        )
        self.meta_model.fit(oof_scaled, y)

        # Calculate ensemble score
        ensemble_pred = self.meta_model.predict_proba(oof_scaled)[:, 1]
        ensemble_score = roc_auc_score(y, ensemble_pred)

        print(f"\nEnsemble CV Score: {ensemble_score:.4f}")

        # Show meta-model weights
        print("\nMeta-model weights:")
        for i, model_name in enumerate(self.models.keys()):
            weight = self.meta_model.coef_[0][i]
            print(f"{model_name:15}: {weight:.4f}")

        return ensemble_score, model_scores

    def predict(self, X_test):
        """Make predictions using the trained ensemble"""

        # Handle missing values
        X_test_filled = X_test.fillna(X_test.median())

        # Get predictions from base models
        base_predictions = np.zeros((len(X_test), len(self.models)))

        for i, (model_name, model) in enumerate(self.models.items()):
            pred = model.predict_proba(X_test_filled)[:, 1]
            base_predictions[:, i] = pred

        # Scale and predict with meta-model
        base_scaled = self.scaler.transform(base_predictions)
        ensemble_pred = self.meta_model.predict_proba(base_scaled)[:, 1]

        return ensemble_pred, base_predictions

# Advanced feature selection for NFL data
def nfl_feature_selection(X, y, max_features=30):
    """
    NFL-specific feature selection combining multiple methods
    """
    from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
    from sklearn.ensemble import RandomForestClassifier

    # Method 1: Statistical tests
    selector_stats = SelectKBest(score_func=f_classif, k='all')
    selector_stats.fit(X.fillna(0), y)
    stats_scores = pd.DataFrame({
        'feature': X.columns,
        'f_score': selector_stats.scores_
    })

    # Method 2: Mutual information
    selector_mi = SelectKBest(score_func=mutual_info_classif, k='all')
    selector_mi.fit(X.fillna(0), y)
    mi_scores = pd.DataFrame({
        'feature': X.columns,
        'mi_score': selector_mi.scores_
    })

    # Method 3: Random Forest importance
    rf = RandomForestClassifier(n_estimators=100, random_state=42)
    rf.fit(X.fillna(0), y)
    rf_scores = pd.DataFrame({
        'feature': X.columns,
        'rf_importance': rf.feature_importances_
    })

    # Combine scores
    combined = stats_scores.merge(mi_scores, on='feature').merge(rf_scores, on='feature')

    # Normalize scores to 0-1 range
    for col in ['f_score', 'mi_score', 'rf_importance']:
        combined[f'{col}_norm'] = (combined[col] - combined[col].min()) / (combined[col].max() - combined[col].min())

    # Combined score (weighted average)
    combined['combined_score'] = (
        combined['f_score_norm'] * 0.3 +
        combined['mi_score_norm'] * 0.4 +
        combined['rf_importance_norm'] * 0.3
    )

    # Select top features
    top_features = combined.nlargest(max_features, 'combined_score')['feature'].tolist()

    print(f"Selected top {len(top_features)} features:")
    for i, feature in enumerate(top_features[:15]):
        score = combined[combined['feature'] == feature]['combined_score'].iloc[0]
        print(f"{i+1:2d}. {feature:25s}: {score:.4f}")

    return top_features, combined

def calculate_position_draft_rates(df):
    """
    Calculate draft rates by position type from NFL dataset
    """
    print("=== Calculating Position Draft Rates ===")

    # Group by Position_Type and calculate draft statistics
    position_stats = df.groupby('Position_Type').agg({
        'Drafted': ['count', 'sum', 'mean']
    }).round(4)

    # Flatten column names
    position_stats.columns = ['Total_Players', 'Players_Drafted', 'Draft_Rate']

    # Sort by draft rate (highest to lowest)
    position_stats = position_stats.sort_values('Draft_Rate', ascending=False)

    print("Position Draft Rate Analysis:")
    print("=" * 60)
    print(f"{'Position Type':<20} {'Total':<8} {'Drafted':<8} {'Rate':<8}")
    print("=" * 60)

    position_rates = {}

    for position, stats in position_stats.iterrows():
        total = int(stats['Total_Players'])
        drafted = int(stats['Players_Drafted'])
        rate = stats['Draft_Rate']

        print(f"{position:<20} {total:<8} {drafted:<8} {rate:.3f}")

        # Store in dictionary for easy use
        position_rates[position] = round(rate, 3)

    print("=" * 60)
    print(f"Overall draft rate: {df['Drafted'].mean():.3f}")

    return position_rates

def create_advanced_features(df):
  # School frequency
  school_freq = df["School"].value_counts(normalize=True)
  df["school_freq"] = df["School"].map(school_freq)

  # Check if 'Id' and 'School' columns exist before dropping
  columns_to_drop = ['Id', 'Year', 'School']
  df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

  cols_to_fill = ['Age', 'Sprint_40yd', 'Vertical_Jump', 'Bench_Press_Reps',
                  'Broad_Jump', 'Agility_3cone', 'Shuttle']
  # train ã®å¹³åã§ train/test ä¸¡æ¹ãè£å®
  for col in cols_to_fill:
      mean_value = df[col].mean()
      df[col] = df[col].fillna(mean_value)
      if col != 'Age':
        df[f'{col}_Missing'] = df[col].isnull().astype(int)


  # ã«ãã´ãªãã¼ã¿ãã©ãã«ã¨ã³ã³ã¼ãã£ã³ã°
  label_encoders = {}
  # for c in ['Player_Type', 'Position_Type', 'Position']:
  for c in ['Player_Type', 'Position_Type', 'Position']:
    label_encoders[c] = LabelEncoder()
    df[c] = label_encoders[c].fit_transform(df[c].astype(str))

  # BMI
  df['BMI'] = round(df['Weight'] / df['Height'] ** 2, 2)

  #ççºå
  # train['Explosiveness'] = round(train['Vertical_Jump'] + train['Broad_Jump'], 2)
  # test['Explosiveness'] = round(test['Vertical_Jump'] + test['Broad_Jump'], 2)

  #éåº¦
  df['Speed_Score'] = round((df['Weight'] * 200) / (df['Sprint_40yd'] ** 4), 2)

  df['High_Draft_Position'] = df['Position_Type'].isin(['line_backer', 'defensive_lineman']).astype(int)
  df['Skill_Position'] = (df['Position_Type'] == 'backs_receivers').astype(int)
  df['Big_Man'] = df['Position_Type'].isin(['offensive_lineman', 'defensive_lineman']).astype(int)
  df['Is_Defense'] = (df['Player_Type'] == 'defense').astype(int)

  return df.copy()

PATH = '/content/drive/My Drive/GCI/NFL/'

train = pd.read_csv(PATH + 'train.csv')
test = pd.read_csv(PATH + 'test.csv')

# Apply advanced feature engineering (from previous artifact)
train_enhanced = create_advanced_features(train)
position_rates = calculate_position_draft_rates(train_enhanced)
train_enhanced['Position_Draft_Rate'] = train_enhanced['Position_Type'].map(position_rates)
train_enhanced['Position_Draft_Rate'] = train_enhanced['Position_Draft_Rate'].fillna(train_enhanced['Drafted'].mean())
# train_enhanced.head()

test_enhanced = create_advanced_features(test)
test_enhanced['Position_Draft_Rate'] = test_enhanced['Position_Type'].map(position_rates)
test_enhanced['Position_Draft_Rate'] = test_enhanced['Position_Draft_Rate'].fillna(train_enhanced['Drafted'].mean())
# test_enhanced.head()

# Feature selection
X = train_enhanced.drop(['Drafted'], axis=1)
y = train_enhanced['Drafted']

# selected_features, feature_scores = nfl_feature_selection(X, y, max_features=25)
# X_selected = X[selected_features]
# test_selected = test_enhanced[selected_features]

# Train ensemble
ensemble = NFLDraftEnsemble(random_state=2025)
ensemble_score, individual_scores = ensemble.train_stacking_ensemble(X, y)

# Make predictions
final_predictions, base_predictions = ensemble.predict(test_enhanced)

print(f"Final ensemble CV score: {ensemble_score:.4f}")