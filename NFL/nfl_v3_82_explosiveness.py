# -*- coding: utf-8 -*-
"""nfl-v3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15-4IEJd2C6x8oAWi09pklOpWGi_fwliT
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# カテゴリ変数を数値化に変換するエンコーダ
from sklearn.preprocessing import LabelEncoder
# ランダムフォレストによる分類器
from sklearn.ensemble import RandomForestClassifier
# 層化K分割交差検証を行うクラス
from sklearn.model_selection import StratifiedKFold
# ROC　AUC　スコアを計算する評価指標
from sklearn.metrics import roc_auc_score

PATH = '/content/drive/My Drive/GCI/NFL/'

train = pd.read_csv(PATH + 'train.csv')
test = pd.read_csv(PATH + 'test.csv')

# print('Train', train.shape)
# print('Test', test.shape)

train.head()
# train.info()
# train.isnull().sum()
# test.isnull().sum()

drafted_counts = train['Drafted'].value_counts()

plt.figure(figsize=(8,6))
plt.bar(drafted_counts.index.astype(str), drafted_counts.values)
plt.title('Drafted Distribution')
plt.xlabel('Drafted', fontsize=14)
plt.ylabel('Count', fontsize=14)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()

drafted_percentage = train['Drafted'].value_counts(normalize=True) * 100
# drafted_percentage
print(f'Percentage of 0: {drafted_percentage.get(0, 0):.2f}%')
print(f'Percentage of 1: {drafted_percentage.get(1, 0):.2f}%')

# 数値列だけを取り出す（ID,Drafted列は除く）
numeric_cols = train.select_dtypes(include=['number']).columns
numeric_cols = numeric_cols.drop(['Id','Drafted'])
# numeric_cols

# プロット
num_cols = len(numeric_cols)
cols = 3
rows = (num_cols + cols - 1) // cols

plt.figure(figsize=(5 * cols, 4 * rows))

for i, col in enumerate(numeric_cols, 1):
    plt.subplot(rows, cols, i)
    plt.hist(train[col].dropna(), bins=30, edgecolor='black')
    plt.title(f'Histgram of {col}')
    plt.xlabel(col)
    plt.ylabel('Frequency')

plt.tight_layout()
plt.show()

# 数値列だけを取り出す（Id,Drafted列を除く）
numeric_cols = train.select_dtypes(include=['number']).drop(['Id','Drafted'], axis=1)

# 相関行列を計算
corr_matrix = numeric_cols.corr()
# corr_matrix

# ヒートマップをプロット
plt.figure(figsize=(12, 10))
sns.heatmap(
    corr_matrix,
    annot=True,
    cmap='coolwarm',
    vmin=-1, vmax=1,
    square=True,
    linewidths=0.5
)
plt.title('Correlation Heatmap')
plt.show()

# 「Sprint_40yd」列と「Broad_Jump」列の相関係数は-0.82で、強い負の相関関係が見られました
# 箱ひげ図を描画
plt.figure(figsize=(10, 6))
sns.boxplot(x=train['Sprint_40yd'])
plt.title('Boxplot of Sprint_40yd', fontsize=16)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.show()
# 「なぜ負の相関があるのか？」という疑問を解決する

# カテゴリデータを抽出
categorical_cols = train.select_dtypes(include=['object', 'category']).columns

# 各列の水準数を獲得
levels_count = {col: train[col].nunique() for col in categorical_cols}

# for col, count in levels_count.items():
#   print(f'{col}: {count} levels')

# Schoolは水準数が236とかなり多く、可視化すると潰れて見づらいのでここでは可視化しません。

# カテゴリデータを抽出
categorical_cols = train.select_dtypes(include=['object', 'category']).columns
categorical_cols = categorical_cols.drop('School')

# グラフ描画準備
num_cols = len(categorical_cols)
rows = 1
cols = num_cols

fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5))

if cols == 1:
  axes = [axes]
else:
  axes = axes.flatten()

# 各カテゴリ変数でカウントプロット
for i, col in enumerate(categorical_cols):
  sns.countplot(x=col, data=train, order=train[col].value_counts().index, ax=axes[i])
  axes[i].set_title(f'Count Plot of {col}', fontsize=14)
  axes[i].set_xlabel(col, fontsize=12)
  axes[i].set_ylabel('Count', fontsize=12)
  axes[i].tick_params(axis='x', rotation=45)
  axes[i].grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

# カテゴリ変数（object型またはcategory型）を抽出し、school列を除外
categorical_cols = train.select_dtypes(include=['object', 'category']).columns
categorical_cols = categorical_cols.drop('School')

# グラフ描画準備
num_cols = len(categorical_cols)
rows = 1
cols = num_cols

fig, axes = plt.subplots(rows, cols, figsize=(5 * cols, 5))

if cols == 1:
  axes = [axes]
else:
  axes = axes.flatten()

# 各カテゴリ変数ごとにDrafted列の平均を棒グラフ描画
for i, col in enumerate(categorical_cols):
  mean_values = train.groupby(col)['Drafted'].mean().sort_values(ascending=False)
  # print(mean_values.index,'---', mean_values.values)
  sns.barplot(x=mean_values.index, y=mean_values.values, ax=axes[i])
  axes[i].set_title(f'Average Drafted by {col}', fontsize=14)
  axes[i].set_xlabel(col, fontsize=12)
  axes[i].set_ylabel('Avarage Drafted', fontsize=12)
  axes[i].set_ylim(0, 1)
  axes[i].tick_params(axis='x', rotation=45)
  axes[i].grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()

train.head()

PATH = '/content/drive/My Drive/GCI/NFL/'

train = pd.read_csv(PATH + 'train.csv')
test = pd.read_csv(PATH + 'test.csv')

# 前処理　preprocessing
# 欠損補完、エンコーディングを行います
# 注意！ここでは「school」を削除しましたがDraftedに関わりかも
# IdとDrafted関係がないと予想される、削除する

# Check if 'Id' and 'School' columns exist before dropping
columns_to_drop = ['Id', 'School']
train = train.drop(columns=[col for col in columns_to_drop if col in train.columns])
test = test.drop(columns=[col for col in columns_to_drop if col in test.columns])

cols_to_fill = ['Age', 'Sprint_40yd', 'Vertical_Jump', 'Bench_Press_Reps',
                'Broad_Jump', 'Agility_3cone', 'Shuttle']
# train の平均で train/test 両方を補完
for col in cols_to_fill:
    mean_value = train[col].mean()
    train[col] = train[col].fillna(mean_value)
    test[col] = test[col].fillna(mean_value)


# カテゴリデータをラベルエンコーディング
label_encoders = {}
for c in ['Player_Type', 'Position_Type', 'Position']:
  label_encoders[c] = LabelEncoder()
  train[c] = label_encoders[c].fit_transform(train[c].astype(str))
  test[c] = label_encoders[c].transform(test[c].astype(str))

# BMI
train['BMI'] = round(train['Weight'] / train['Height'] ** 2, 2)
test['BMI'] = round(test['Weight'] / test['Height'] ** 2, 2)

#爆発力
train['Explosiveness'] = round(train['Vertical_Jump'] + train['Broad_Jump'], 2)
test['Explosiveness'] = round(test['Vertical_Jump'] + test['Broad_Jump'], 2)

#速度
train['Speed_Score'] = round((train['Weight'] * 200) / (train['Sprint_40yd'] ** 4), 2)
test['Speed_Score'] = round((test['Weight'] * 200) / (test['Sprint_40yd'] ** 4), 2)

train.head()

# モデル構築

# 特徴量と目的変数に分ける
X = train.drop(columns=['Drafted'])
y = train['Drafted']

# モデルとCrossValidationの設定
model = RandomForestClassifier(
    n_estimators=100,
    max_depth=5,
    random_state=2025
)
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# スコア・予測格納用
auc_scores = []
test_pred_proba_list = []

# Stratified K-Fold による学習と評価
for fold, (train_idx, valid_idx) in enumerate(skf.split(X, y)):
  print(f'Fold {fold + 1}')

  X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]
  y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]

  # モデル学習
  model.fit(X_train, y_train)

  # バリデーション予測　と　スコア
  y_valid_pred_proba = model.predict_proba(X_valid)[:, 1]
  auc = roc_auc_score(y_valid, y_valid_pred_proba)
  auc_scores.append(auc)
  print(f'AUC: {round(auc, 4)}')

  # テストデータ予測を保存
  test_pred_proba = model.predict_proba(test)[:, 1]
  test_pred_proba_list.append(test_pred_proba)

# 平均AUCを表示
mean_auc = np.mean(auc_scores)
print(f'Mean AUC: {round(mean_auc, 4)}')

# テスト予測の平均を計算
test_pred_proba_mean = np.mean(test_pred_proba_list, axis=0)

# best_fold_idx = int(np.argmax(auc_scores))
# test_pred_proba_best = test_pred_proba_list[best_fold_idx]
# print(best_fold_idx)

# 特徴量とその重要度をDataFrameにまとめる
feature_importances = pd.DataFrame({
    'Feature': X.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Feature Importance', fontsize=16)
plt.xlabel('Importance', fontsize=14)
plt.ylabel('Feature', fontsize=14)
plt.tight_layout()
plt.show()

# ファイル作成
test_pred_proba_mean
submission = pd.read_csv(PATH + 'sample_submission.csv')
submission['Drafted'] = test_pred_proba_mean
submission.to_csv(PATH + 'v3_submission.csv', index=False)