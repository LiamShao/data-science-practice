import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import stats
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

class HypothesisDrivenEDA:
    def __init__(self, df):
        self.df = df.copy()
        self.results = {}
        
    def test_h1_department_job_attrition(self):
        """H1: æŸäº›å²—ä½/éƒ¨é—¨æµå¤±ç‡æ˜¾è‘—é«˜äºå…¶ä»–"""
        print("="*60)
        print("H1 éªŒè¯: å²—ä½/éƒ¨é—¨æµå¤±ç‡å·®å¼‚åˆ†æ")
        print("="*60)
        
        # 1. éƒ¨é—¨æµå¤±ç‡åˆ†æ
        dept_attrition = self.df.groupby('Department').agg({
            'Attrition': [
                lambda x: (x == 'Yes').mean(),  # æµå¤±ç‡
                lambda x: (x == 'Yes').sum(),   # æµå¤±äººæ•°
                'count'                         # æ€»äººæ•°
            ]
        }).round(4)
        
        dept_attrition.columns = ['AttritionRate', 'AttritionCount', 'TotalCount']
        dept_attrition = dept_attrition.sort_values('AttritionRate', ascending=False)
        
        print("\nğŸ“Š å„éƒ¨é—¨æµå¤±ç‡:")
        print(dept_attrition)
        
        # 2. å²—ä½æµå¤±ç‡åˆ†æ
        job_attrition = self.df.groupby('JobRole').agg({
            'Attrition': [
                lambda x: (x == 'Yes').mean(),
                lambda x: (x == 'Yes').sum(),
                'count'
            ]
        }).round(4)
        
        job_attrition.columns = ['AttritionRate', 'AttritionCount', 'TotalCount']
        job_attrition = job_attrition.sort_values('AttritionRate', ascending=False)
        
        print("\nğŸ“Š å„å²—ä½æµå¤±ç‡ (å‰10):")
        print(job_attrition.head(10))
        
        # 3. ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
        overall_attrition_rate = (self.df['Attrition'] == 'Yes').mean()
        print(f"\nğŸ“ˆ æ•´ä½“æµå¤±ç‡: {overall_attrition_rate:.2%}")
        
        # å¡æ–¹æ£€éªŒ
        from scipy.stats import chi2_contingency
        contingency_dept = pd.crosstab(self.df['Department'], self.df['Attrition'])
        chi2_dept, p_value_dept, dof, expected = chi2_contingency(contingency_dept)
        
        print(f"\nğŸ”¬ ç»Ÿè®¡æ£€éªŒç»“æœ:")
        print(f"éƒ¨é—¨å·®å¼‚å¡æ–¹æ£€éªŒ p-value: {p_value_dept:.4f}")
        
        # 4. å¯è§†åŒ–
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # éƒ¨é—¨æµå¤±ç‡
        dept_attrition['AttritionRate'].plot(kind='bar', ax=ax1, color='skyblue')
        ax1.set_title('å„éƒ¨é—¨æµå¤±ç‡')
        ax1.set_ylabel('æµå¤±ç‡')
        ax1.axhline(y=overall_attrition_rate, color='red', linestyle='--', 
                   label=f'æ•´ä½“å¹³å‡({overall_attrition_rate:.2%})')
        ax1.legend()
        ax1.tick_params(axis='x', rotation=45)
        
        # å²—ä½æµå¤±ç‡ (å‰8ä¸ª)
        job_attrition.head(8)['AttritionRate'].plot(kind='bar', ax=ax2, color='lightcoral')
        ax2.set_title('å„å²—ä½æµå¤±ç‡ (Top 8)')
        ax2.set_ylabel('æµå¤±ç‡')
        ax2.axhline(y=overall_attrition_rate, color='red', linestyle='--', 
                   label=f'æ•´ä½“å¹³å‡({overall_attrition_rate:.2%})')
        ax2.legend()
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
        
        # 5. ç»“è®º
        max_dept_attrition = dept_attrition['AttritionRate'].max()
        min_dept_attrition = dept_attrition['AttritionRate'].min()
        
        h1_conclusion = {
            'hypothesis': 'H1: æŸäº›å²—ä½/éƒ¨é—¨æµå¤±ç‡æ˜¾è‘—é«˜äºå…¶ä»–',
            'p_value': p_value_dept,
            'significant': p_value_dept < 0.05,
            'max_dept_rate': max_dept_attrition,
            'min_dept_rate': min_dept_attrition,
            'rate_difference': max_dept_attrition - min_dept_attrition,
            'conclusion': f"éƒ¨é—¨é—´æµå¤±ç‡å·®å¼‚{'æ˜¾è‘—' if p_value_dept < 0.05 else 'ä¸æ˜¾è‘—'}"
        }
        
        self.results['H1'] = h1_conclusion
        print(f"\nâœ… H1ç»“è®º: {h1_conclusion['conclusion']}")
        print(f"   æœ€é«˜æµå¤±ç‡: {max_dept_attrition:.2%}, æœ€ä½æµå¤±ç‡: {min_dept_attrition:.2%}")
        
        return h1_conclusion
    
    def test_h2_compensation_performance_mismatch(self):
        """H2: è–ªé…¬ä¸ç»©æ•ˆå­˜åœ¨ä¸åŒ¹é…ç°è±¡"""
        print("\n" + "="*60)
        print("H2 éªŒè¯: è–ªé…¬ä¸ç»©æ•ˆåŒ¹é…åº¦åˆ†æ")
        print("="*60)
        
        # 1. è–ªé…¬ä¸ç»©æ•ˆç›¸å…³æ€§
        correlation = self.df['MonthlyIncome'].corr(self.df['PerformanceRating'])
        print(f"\nğŸ“Š è–ªé…¬ä¸ç»©æ•ˆç›¸å…³ç³»æ•°: {correlation:.4f}")
        
        # 2. æŒ‰ç»©æ•ˆç­‰çº§åˆ†ç»„çš„è–ªé…¬åˆ†æ
        perf_salary = self.df.groupby('PerformanceRating')['MonthlyIncome'].agg([
            'mean', 'median', 'std', 'count'
        ]).round(0)
        
        print(f"\nğŸ“ˆ å„ç»©æ•ˆç­‰çº§è–ªé…¬ç»Ÿè®¡:")
        print(perf_salary)
        
        # 3. è¯†åˆ«è–ªé…¬å¼‚å¸¸æƒ…å†µ
        # é«˜ç»©æ•ˆä½è–ªé…¬ vs ä½ç»©æ•ˆé«˜è–ªé…¬
        high_perf_threshold = self.df['PerformanceRating'].quantile(0.75)
        low_perf_threshold = self.df['PerformanceRating'].quantile(0.25)
        high_salary_threshold = self.df['MonthlyIncome'].quantile(0.75)
        low_salary_threshold = self.df['MonthlyIncome'].quantile(0.25)
        
        # ä¸åŒ¹é…æƒ…å†µ
        high_perf_low_pay = self.df[
            (self.df['PerformanceRating'] >= high_perf_threshold) & 
            (self.df['MonthlyIncome'] <= low_salary_threshold)
        ]
        
        low_perf_high_pay = self.df[
            (self.df['PerformanceRating'] <= low_perf_threshold) & 
            (self.df['MonthlyIncome'] >= high_salary_threshold)
        ]
        
        print(f"\nğŸš¨ è–ªé…¬ä¸åŒ¹é…æƒ…å†µ:")
        print(f"é«˜ç»©æ•ˆä½è–ªé…¬å‘˜å·¥: {len(high_perf_low_pay)} äºº ({len(high_perf_low_pay)/len(self.df):.1%})")
        print(f"ä½ç»©æ•ˆé«˜è–ªé…¬å‘˜å·¥: {len(low_perf_high_pay)} äºº ({len(low_perf_high_pay)/len(self.df):.1%})")
        
        # 4. ä¸åŒ¹é…å‘˜å·¥çš„æµå¤±ç‡
        if len(high_perf_low_pay) > 0:
            high_perf_low_pay_attrition = (high_perf_low_pay['Attrition'] == 'Yes').mean()
            print(f"é«˜ç»©æ•ˆä½è–ªé…¬å‘˜å·¥æµå¤±ç‡: {high_perf_low_pay_attrition:.2%}")
        
        if len(low_perf_high_pay) > 0:
            low_perf_high_pay_attrition = (low_perf_high_pay['Attrition'] == 'Yes').mean()
            print(f"ä½ç»©æ•ˆé«˜è–ªé…¬å‘˜å·¥æµå¤±ç‡: {low_perf_high_pay_attrition:.2%}")
        
        # 5. å¯è§†åŒ–
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # æ•£ç‚¹å›¾ï¼šè–ªé…¬ vs ç»©æ•ˆ
        colors = ['red' if x == 'Yes' else 'blue' for x in self.df['Attrition']]
        ax1.scatter(self.df['PerformanceRating'], self.df['MonthlyIncome'], 
                   c=colors, alpha=0.6)
        ax1.set_xlabel('ç»©æ•ˆè¯„çº§')
        ax1.set_ylabel('æœˆè–ª')
        ax1.set_title(f'è–ªé…¬ vs ç»©æ•ˆæ•£ç‚¹å›¾\n(ç›¸å…³ç³»æ•°: {correlation:.3f})')
        
        # ç®±çº¿å›¾ï¼šä¸åŒç»©æ•ˆç­‰çº§çš„è–ªé…¬åˆ†å¸ƒ
        self.df.boxplot(column='MonthlyIncome', by='PerformanceRating', ax=ax2)
        ax2.set_title('å„ç»©æ•ˆç­‰çº§è–ªé…¬åˆ†å¸ƒ')
        ax2.set_xlabel('ç»©æ•ˆè¯„çº§')
        ax2.set_ylabel('æœˆè–ª')
        
        plt.tight_layout()
        plt.show()
        
        # 6. ç»“è®º
        mismatch_rate = (len(high_perf_low_pay) + len(low_perf_high_pay)) / len(self.df)
        
        h2_conclusion = {
            'hypothesis': 'H2: è–ªé…¬ä¸ç»©æ•ˆå­˜åœ¨ä¸åŒ¹é…ç°è±¡',
            'correlation': correlation,
            'mismatch_rate': mismatch_rate,
            'high_perf_low_pay_count': len(high_perf_low_pay),
            'low_perf_high_pay_count': len(low_perf_high_pay),
            'significant_mismatch': mismatch_rate > 0.1,
            'conclusion': f"è–ªé…¬ç»©æ•ˆä¸åŒ¹é…æ¯”ä¾‹: {mismatch_rate:.1%}"
        }
        
        self.results['H2'] = h2_conclusion
        print(f"\nâœ… H2ç»“è®º: {h2_conclusion['conclusion']}")
        
        return h2_conclusion
    
    def test_h3_satisfaction_drives_attrition(self):
        """H3: å·¥ä½œæ»¡æ„åº¦æ˜¯æµå¤±çš„ä¸»è¦é©±åŠ¨å› ç´ """
        print("\n" + "="*60)
        print("H3 éªŒè¯: æ»¡æ„åº¦ä¸æµå¤±å…³ç³»åˆ†æ")
        print("="*60)
        
        satisfaction_cols = ['JobSatisfaction', 'EnvironmentSatisfaction', 
                           'RelationshipSatisfaction', 'WorkLifeBalance']
        
        # 1. å„æ»¡æ„åº¦æŒ‡æ ‡ä¸æµå¤±çš„å…³ç³»
        satisfaction_analysis = {}
        
        for col in satisfaction_cols:
            if col in self.df.columns:
                # ç•™ä»»vsç¦»èŒå‘˜å·¥çš„æ»¡æ„åº¦å¯¹æ¯”
                stay_avg = self.df[self.df['Attrition'] == 'No'][col].mean()
                leave_avg = self.df[self.df['Attrition'] == 'Yes'][col].mean()
                difference = stay_avg - leave_avg
                
                # tæ£€éªŒ
                stay_scores = self.df[self.df['Attrition'] == 'No'][col].dropna()
                leave_scores = self.df[self.df['Attrition'] == 'Yes'][col].dropna()
                t_stat, p_value = stats.ttest_ind(stay_scores, leave_scores)
                
                satisfaction_analysis[col] = {
                    'stay_avg': stay_avg,
                    'leave_avg': leave_avg,
                    'difference': difference,
                    'p_value': p_value,
                    'significant': p_value < 0.05
                }
                
                print(f"\nğŸ“Š {col}:")
                print(f"   ç•™ä»»å‘˜å·¥å¹³å‡åˆ†: {stay_avg:.2f}")
                print(f"   ç¦»èŒå‘˜å·¥å¹³å‡åˆ†: {leave_avg:.2f}")
                print(f"   å·®å¼‚: {difference:.2f} (p-value: {p_value:.4f})")
        
        # 2. æ»¡æ„åº¦ç­‰çº§ä¸æµå¤±ç‡äº¤å‰åˆ†æ
        print(f"\nğŸ“ˆ å„æ»¡æ„åº¦ç­‰çº§æµå¤±ç‡:")
        for col in satisfaction_cols:
            if col in self.df.columns:
                satisfaction_attrition = self.df.groupby(col)['Attrition'].apply(
                    lambda x: (x == 'Yes').mean()
                ).round(4)
                print(f"\n{col}:")
                print(satisfaction_attrition)
        
        # 3. ç»¼åˆæ»¡æ„åº¦å¾—åˆ†
        if all(col in self.df.columns for col in satisfaction_cols):
            self.df['OverallSatisfaction'] = self.df[satisfaction_cols].mean(axis=1)
            
            # æ»¡æ„åº¦åˆ†ç»„
            self.df['SatisfactionLevel'] = pd.cut(
                self.df['OverallSatisfaction'], 
                bins=[0, 2, 3, 4, 5], 
                labels=['ä½', 'ä¸­ä¸‹', 'ä¸­ä¸Š', 'é«˜']
            )
            
            satisfaction_level_attrition = self.df.groupby('SatisfactionLevel')['Attrition'].apply(
                lambda x: (x == 'Yes').mean()
            ).round(4)
            
            print(f"\nğŸ“Š ç»¼åˆæ»¡æ„åº¦ç­‰çº§æµå¤±ç‡:")
            print(satisfaction_level_attrition)
        
        # 4. å¯è§†åŒ–
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        axes = axes.flatten()
        
        for i, col in enumerate(satisfaction_cols):
            if col in self.df.columns and i < 4:
                # ç®±çº¿å›¾
                self.df.boxplot(column=col, by='Attrition', ax=axes[i])
                axes[i].set_title(f'{col} vs æµå¤±')
                axes[i].set_xlabel('æ˜¯å¦æµå¤±')
                axes[i].set_ylabel('æ»¡æ„åº¦è¯„åˆ†')
        
        plt.tight_layout()
        plt.show()
        
        # 5. æ»¡æ„åº¦é‡è¦æ€§æ’åº
        importance_ranking = []
        for col, analysis in satisfaction_analysis.items():
            importance_ranking.append({
                'satisfaction_type': col,
                'effect_size': abs(analysis['difference']),
                'p_value': analysis['p_value'],
                'significant': analysis['significant']
            })
        
        importance_df = pd.DataFrame(importance_ranking).sort_values(
            'effect_size', ascending=False
        )
        
        print(f"\nğŸ† æ»¡æ„åº¦å½±å“åŠ›æ’åº:")
        print(importance_df)
        
        # 6. ç»“è®º
        significant_factors = sum(1 for analysis in satisfaction_analysis.values() 
                                if analysis['significant'])
        
        h3_conclusion = {
            'hypothesis': 'H3: å·¥ä½œæ»¡æ„åº¦æ˜¯æµå¤±çš„ä¸»è¦é©±åŠ¨å› ç´ ',
            'significant_factors': significant_factors,
            'top_factor': importance_df.iloc[0]['satisfaction_type'] if len(importance_df) > 0 else None,
            'max_effect_size': importance_df.iloc[0]['effect_size'] if len(importance_df) > 0 else 0,
            'satisfaction_analysis': satisfaction_analysis,
            'strong_evidence': significant_factors >= 3
        }
        
        self.results['H3'] = h3_conclusion
        print(f"\nâœ… H3ç»“è®º: å‘ç°{significant_factors}ä¸ªæ˜¾è‘—çš„æ»¡æ„åº¦å½±å“å› ç´ ")
        
        return h3_conclusion
    
    def test_h4_flexible_work_improves_retention(self):
        """H4: è¿œç¨‹å·¥ä½œ/å¼¹æ€§å·¥ä½œèƒ½æ”¹å–„å‘˜å·¥ä¿ç•™"""
        print("\n" + "="*60)
        print("H4 éªŒè¯: å·¥ä½œæ¨¡å¼ä¸å‘˜å·¥ä¿ç•™å…³ç³»")
        print("="*60)
        
        work_flexibility_cols = ['RemoteWork', 'FlexibleWork', 'OverTime']
        
        # 1. å„å·¥ä½œæ¨¡å¼çš„æµå¤±ç‡å¯¹æ¯”
        for col in work_flexibility_cols:
            if col in self.df.columns:
                work_mode_attrition = self.df.groupby(col)['Attrition'].apply(
                    lambda x: (x == 'Yes').mean()
                ).round(4)
                
                print(f"\nğŸ“Š {col} æµå¤±ç‡:")
                print(work_mode_attrition)
                
                # å¡æ–¹æ£€éªŒ
                contingency = pd.crosstab(self.df[col], self.df['Attrition'])
                chi2, p_value, dof, expected = chi2_contingency(contingency)
                print(f"   ç»Ÿè®¡æ˜¾è‘—æ€§ p-value: {p_value:.4f}")
        
        # 2. å·¥ä½œæ»¡æ„åº¦ä¸å·¥ä½œæ¨¡å¼çš„å…³ç³»
        if 'RemoteWork' in self.df.columns and 'JobSatisfaction' in self.df.columns:
            remote_satisfaction = self.df.groupby('RemoteWork')['JobSatisfaction'].mean()
            print(f"\nğŸ“ˆ è¿œç¨‹å·¥ä½œä¸å·¥ä½œæ»¡æ„åº¦:")
            print(remote_satisfaction.round(2))
        
        # 3. åŠ ç­ä¸æµå¤±å…³ç³»æ·±åº¦åˆ†æ
        if 'OverTime' in self.df.columns:
            overtime_analysis = self.df.groupby(['OverTime', 'Attrition']).size().unstack()
            overtime_rates = self.df.groupby('OverTime')['Attrition'].apply(
                lambda x: (x == 'Yes').mean()
            )
            
            print(f"\nğŸ” åŠ ç­æƒ…å†µè¯¦ç»†åˆ†æ:")
            print(f"åŠ ç­å‘˜å·¥æµå¤±ç‡: {overtime_rates.get('Yes', 0):.2%}")
            print(f"ä¸åŠ ç­å‘˜å·¥æµå¤±ç‡: {overtime_rates.get('No', 0):.2%}")
        
        # 4. å¯è§†åŒ–
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        for i, col in enumerate(work_flexibility_cols):
            if col in self.df.columns and i < 3:
                # æµå¤±ç‡æ¡å½¢å›¾
                attrition_by_mode = self.df.groupby(col)['Attrition'].apply(
                    lambda x: (x == 'Yes').mean()
                )
                attrition_by_mode.plot(kind='bar', ax=axes[i], color='lightcoral')
                axes[i].set_title(f'{col} vs æµå¤±ç‡')
                axes[i].set_ylabel('æµå¤±ç‡')
                axes[i].tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.show()
        
        # 5. ç»“è®º
        flexibility_effects = {}
        for col in work_flexibility_cols:
            if col in self.df.columns:
                mode_rates = self.df.groupby(col)['Attrition'].apply(
                    lambda x: (x == 'Yes').mean()
                )
                flexibility_effects[col] = {
                    'rates': mode_rates.to_dict(),
                    'beneficial': any(rate < 0.15 for rate in mode_rates.values())
                }
        
        h4_conclusion = {
            'hypothesis': 'H4: è¿œç¨‹å·¥ä½œ/å¼¹æ€§å·¥ä½œèƒ½æ”¹å–„å‘˜å·¥ä¿ç•™',
            'flexibility_effects': flexibility_effects,
            'supports_hypothesis': any(effect['beneficial'] for effect in flexibility_effects.values())
        }
        
        self.results['H4'] = h4_conclusion
        print(f"\nâœ… H4ç»“è®º: å·¥ä½œçµæ´»æ€§å¯¹å‘˜å·¥ä¿ç•™çš„å½±å“åˆ†æå®Œæˆ")
        
        return h4_conclusion
    
    def test_h5_training_development_correlation(self):
        """H5: åŸ¹è®­æŠ•å…¥ä¸å‘˜å·¥å‘å±•æ­£ç›¸å…³"""
        print("\n" + "="*60)
        print("H5 éªŒè¯: åŸ¹è®­æŠ•å…¥ä¸å‘˜å·¥å‘å±•å…³ç³»")
        print("="*60)
        
        # 1. åŸ¹è®­æ¬¡æ•°ä¸ç›¸å…³æŒ‡æ ‡çš„å…³ç³»
        if 'TrainingTimesLastYear' in self.df.columns:
            print(f"\nğŸ“Š åŸ¹è®­æ¬¡æ•°åŸºæœ¬ç»Ÿè®¡:")
            print(self.df['TrainingTimesLastYear'].describe())
            
            # åŸ¹è®­æ¬¡æ•°ä¸æµå¤±å…³ç³»
            training_attrition = self.df.groupby('TrainingTimesLastYear')['Attrition'].apply(
                lambda x: (x == 'Yes').mean()
            ).round(4)
            
            print(f"\nğŸ“ˆ å„åŸ¹è®­æ¬¡æ•°æµå¤±ç‡:")
            print(training_attrition)
            
            # åŸ¹è®­æ¬¡æ•°ä¸ç»©æ•ˆå…³ç³»
            if 'PerformanceRating' in self.df.columns:
                training_performance_corr = self.df['TrainingTimesLastYear'].corr(
                    self.df['PerformanceRating']
                )
                print(f"\nğŸ“Š åŸ¹è®­æ¬¡æ•°ä¸ç»©æ•ˆç›¸å…³ç³»æ•°: {training_performance_corr:.4f}")
            
            # åŸ¹è®­æ¬¡æ•°ä¸æ»¡æ„åº¦å…³ç³»
            if 'JobSatisfaction' in self.df.columns:
                training_satisfaction_corr = self.df['TrainingTimesLastYear'].corr(
                    self.df['JobSatisfaction']
                )
                print(f"åŸ¹è®­æ¬¡æ•°ä¸å·¥ä½œæ»¡æ„åº¦ç›¸å…³ç³»æ•°: {training_satisfaction_corr:.4f}")
            
            # åŸ¹è®­æ¬¡æ•°ä¸èŒä¸šå‘å±•å…³ç³»
            development_cols = ['YearsSinceLastPromotion', 'YearsInCurrentRole']
            
            for col in development_cols:
                if col in self.df.columns:
                    corr = self.df['TrainingTimesLastYear'].corr(self.df[col])
                    print(f"åŸ¹è®­æ¬¡æ•°ä¸{col}ç›¸å…³ç³»æ•°: {corr:.4f}")
        
        # 2. åŸ¹è®­æŠ•å…¥åˆ†ç»„åˆ†æ
        if 'TrainingTimesLastYear' in self.df.columns:
            # åˆ›å»ºåŸ¹è®­æŠ•å…¥ç­‰çº§
            self.df['TrainingLevel'] = pd.cut(
                self.df['TrainingTimesLastYear'],
                bins=[-1, 0, 2, 4, 10],
                labels=['æ— åŸ¹è®­', 'å°‘é‡åŸ¹è®­', 'é€‚ä¸­åŸ¹è®­', 'å¤§é‡åŸ¹è®­']
            )
            
            training_level_analysis = self.df.groupby('TrainingLevel').agg({
                'Attrition': lambda x: (x == 'Yes').mean(),
                'PerformanceRating': 'mean',
                'JobSatisfaction': 'mean',
                'MonthlyIncome': 'mean'
            }).round(3)
            
            print(f"\nğŸ“Š ä¸åŒåŸ¹è®­æ°´å¹³å‘˜å·¥è¡¨ç°:")
            print(training_level_analysis)
        
        # 3. å¯è§†åŒ–
        if 'TrainingTimesLastYear' in self.df.columns:
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            
            # åŸ¹è®­æ¬¡æ•°åˆ†å¸ƒ
            self.df['TrainingTimesLastYear'].hist(bins=20, ax=axes[0,0])
            axes[0,0].set_title('åŸ¹è®­æ¬¡æ•°åˆ†å¸ƒ')
            axes[0,0].set_xlabel('åŸ¹è®­æ¬¡æ•°')
            axes[0,0].set_ylabel('å‘˜å·¥æ•°é‡')
            
            # åŸ¹è®­æ¬¡æ•°vsæµå¤±ç‡
            if len(training_attrition) > 1:
                training_attrition.plot(kind='bar', ax=axes[0,1], color='skyblue')
                axes[0,1].set_title('åŸ¹è®­æ¬¡æ•° vs æµå¤±ç‡')
                axes[0,1].set_ylabel('æµå¤±ç‡')
                axes[0,1].tick_params(axis='x', rotation=45)
            
            # åŸ¹è®­æ°´å¹³vsç»©æ•ˆ
            if 'TrainingLevel' in self.df.columns and 'PerformanceRating' in self.df.columns:
                self.df.boxplot(column='PerformanceRating', by='TrainingLevel', ax=axes[1,0])
                axes[1,0].set_title('åŸ¹è®­æ°´å¹³ vs ç»©æ•ˆè¯„çº§')
                axes[1,0].tick_params(axis='x', rotation=45)
            
            # åŸ¹è®­æ¬¡æ•°vsè–ªé…¬æ•£ç‚¹å›¾
            if 'MonthlyIncome' in self.df.columns:
                axes[1,1].scatter(self.df['TrainingTimesLastYear'], self.df['MonthlyIncome'], alpha=0.6)
                axes[1,1].set_title('åŸ¹è®­æ¬¡æ•° vs æœˆè–ª')
                axes[1,1].set_xlabel('åŸ¹è®­æ¬¡æ•°')
                axes[1,1].set_ylabel('æœˆè–ª')
            
            plt.tight_layout()
            plt.show()
        
        # 4. ç»“è®º
        h5_conclusion = {
            'hypothesis': 'H5: åŸ¹è®­æŠ•å…¥ä¸å‘˜å·¥å‘å±•æ­£ç›¸å…³',
            'training_available': 'TrainingTimesLastYear' in self.df.columns,
            'positive_correlations': [],
            'supports_hypothesis': False
        }
        
        if 'TrainingTimesLastYear' in self.df.columns:
            # æ£€æŸ¥æ­£ç›¸å…³å…³ç³»
            correlations = {}
            for col in ['PerformanceRating', 'JobSatisfaction', 'MonthlyIncome']:
                if col in self.df.columns:
                    corr = self.df['TrainingTimesLastYear'].corr(self.df[col])
                    correlations[col] = corr
                    if corr > 0.1:  # é˜ˆå€¼å¯è°ƒæ•´
                        h5_conclusion['positive_correlations'].append(col)
            
            h5_conclusion['correlations'] = correlations
            h5_conclusion['supports_hypothesis'] = len(h5_conclusion['positive_correlations']) >= 2
        
        self.results['H5'] = h5_conclusion
        print(f"\nâœ… H5ç»“è®º: åŸ¹è®­ä¸å‘å±•ç›¸å…³æ€§åˆ†æå®Œæˆ")
        
        return h5_conclusion
    
    def generate_hypothesis_summary(self):
        """ç”Ÿæˆå‡è®¾éªŒè¯æ€»ç»“æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("                    å‡è®¾éªŒè¯æ€»ç»“æŠ¥å‘Š")
        print("="*80)
        
        for hypothesis, result in self.results.items():
            print(f"\n{result['hypothesis']}")
            print("-" * len(result['hypothesis']))
            
            if hypothesis == 'H1':
                status = "âœ… æ”¯æŒ" if result['significant'] else "âŒ ä¸æ”¯æŒ"
                print(f"ç»“è®º: {status}")
                print(f"è¯æ®: éƒ¨é—¨é—´æµå¤±ç‡å·®å¼‚è¾¾{result['rate_difference']:.1%}")
                
            elif hypothesis == 'H2':
                status = "âœ… æ”¯æŒ" if result['significant_mismatch'] else "âŒ ä¸æ”¯æŒ"
                print(f"ç»“è®º: {status}")
                print(f"è¯æ®: {result['mismatch_rate']:.1%}çš„å‘˜å·¥å­˜åœ¨è–ªé…¬ç»©æ•ˆä¸åŒ¹é…")
                
            elif hypothesis == 'H3':
                status = "âœ… å¼ºæ”¯æŒ" if result['strong_evidence'] else "âš ï¸ éƒ¨åˆ†æ”¯æŒ"
                print(f"ç»“è®º: {status}")
                print(f"è¯æ®: {result['significant_factors']}ä¸ªæ»¡æ„åº¦å› ç´ æ˜¾è‘—å½±å“æµå¤±")
                
            elif hypothesis == 'H4':
                status = "âœ… æ”¯æŒ" if result['supports_hypothesis'] else "âŒ ä¸æ”¯æŒ"
                print(f"ç»“è®º: {status}")
                
            elif hypothesis == 'H5':
                if result['training_available']:
                    status = "âœ… æ”¯æŒ" if result['supports_hypothesis'] else "âŒ ä¸æ”¯æŒ"
                    print(f"ç»“è®º: {status}")
                    print(f"è¯æ®: {len(result['positive_correlations'])}ä¸ªæŒ‡æ ‡ä¸åŸ¹è®­æ­£ç›¸å…³")
                else:
                    print("ç»“è®º: âš ï¸ æ•°æ®ä¸è¶³")
        
        # æ•´ä½“æ´å¯Ÿ
        print(f"\n" + "="*50)
        print("ğŸ¯ æ ¸å¿ƒæ´å¯Ÿä¸å»ºè®®")
        print("="*50)
        
        supported_hypotheses = sum(1 for result in self.results.values() 
                                 if result.get('significant', False) or 
                                    result.get('supports_hypothesis', False) or
                                    result.get('strong_evidence', False))
        
        print(f"âœ… éªŒè¯é€šè¿‡çš„å‡è®¾: {supported_hypotheses}/5")
        print(f"\nğŸ’¡ å…³é”®å‘ç°:")
        
        # åŸºäºéªŒè¯ç»“æœç”Ÿæˆå»ºè®®
        recommendations = []
        
        if self.results.get('H1', {}).get('significant', False):
            recommendations.append("1. é’ˆå¯¹é«˜æµå¤±éƒ¨é—¨åˆ¶å®šä¸“é¡¹ä¿ç•™ç­–ç•¥")
        
        if self.results.get('H2', {}).get('significant_mismatch', False):
            recommendations.append("2. å»ºç«‹è–ªé…¬ä¸ç»©æ•ˆè”åŠ¨æœºåˆ¶")
        
        if self.results.get('H3', {}).get('strong_evidence', False):
            recommendations.append("3. å®æ–½å‘˜å·¥æ»¡æ„åº¦æå‡è®¡åˆ’")
        
        if self.results.get('H4', {}).get('supports_hypothesis', False):
            recommendations.append("4. æ¨å¹¿çµæ´»å·¥ä½œåˆ¶åº¦")
        
        if self.results.get('H5', {}).get('supports_hypothesis', False):
            recommendations.append("5. åŠ å¤§å‘˜å·¥åŸ¹è®­æŠ•å…¥")
        
        for rec in recommendations:
            print(f"   {rec}")
        
        return self.results

# ä½¿ç”¨ç¤ºä¾‹å‡½æ•°
def run_hypothesis_driven_eda(df):
    """è¿è¡Œå®Œæ•´çš„å‡è®¾é©±åŠ¨EDAæµç¨‹"""
    
    print("ğŸš€ å¼€å§‹å‡è®¾é©±åŠ¨çš„EDAåˆ†æ...")
    print("æ•°æ®é›†åŸºæœ¬ä¿¡æ¯:")
    print(f"   è¡Œæ•°: {len(df)}")
    print(f"   åˆ—æ•°: {len(df.columns)}")
    print(f"   ç›®æ ‡å˜é‡(Attrition)åˆ†å¸ƒ: {df['Attrition'].value_counts().to_dict()}")
    
    # åˆ›å»ºåˆ†æå™¨å®ä¾‹
    analyzer = HypothesisDrivenEDA(df)
    
    # é€ä¸€éªŒè¯å‡è®¾
    print("\nğŸ”¬ å¼€å§‹å‡è®¾éªŒè¯...")
    
    try:
        analyzer.test_h1_department_job_attrition()
        analyzer.test_h2_compensation_performance_mismatch()
        analyzer.test_h3_satisfaction_drives_attrition()
        analyzer.test_h4_flexible_work_improves_retention()
        analyzer.test_h5_training_development_correlation()
        
        # ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
        results = analyzer.generate_hypothesis_summary()
        
        return results
        
    except Exception as e:
        print(f"åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        return None

# è¿è¡Œåˆ†æçš„ç¤ºä¾‹ä»£ç 
# results = run_hypothesis_driven_eda(your_dataframe)